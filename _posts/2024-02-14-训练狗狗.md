---
title: 训练狗狗捡起我们扔的棍子
date: 2024-02-14 10:21:00 +0800

img_path: "/assets/img/posts/2024-02-14-训练狗狗"
categories: [深度学习]
tags: [实验]
---

> 跑模型时间！
>
> 来自 Hugging Face 强化学习课程。
{: .prompt-info }

Huggy 是 Hugging Face 制作的深度强化学习环境，基于[Unity MLAgents 团队的项目 Puppo the Corgi](https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit)。该环境是使用[Unity 游戏引擎](https://unity.com/)和[MLAgents](https://github.com/Unity-Technologies/ml-agents)创建的。ML-Agents 是 Unity 游戏引擎的工具包，它允许我们使用**Unity 创建环境或使用预制环境来训练我们的智能体**。

![](huggy.gif)

在这个环境中，我们的目标是训练哈吉捡起**我们扔的棍子。这意味着他需要正确地朝棍子移动**。

## 任务

我们向他提供有关环境的信息，或者说**观察**：

- 目标（摇杆）位置
- 自己与目标的相对位置
- 他的腿的方向。

有了所有这些信息，哈吉就可以**使用他的策略来确定下一步要采取哪些行动来实现他的目标**。

动作空间即**关节电机驱动哈吉的腿**。这意味着为了获得目标，哈吉需要学会**正确旋转每条腿的关节电机，这样他才能移动**。

![](huggy-action.jpg)

奖励函数的设计是为了让**Huggy 实现他的目标**：拿起棍子。

- *定向奖励*：我们**奖励他接近目标**。
- *时间惩罚*：每次动作都会受到固定时间的惩罚，**迫使他尽快到达棍子位置**。
- *旋转惩罚*：如果哈吉**旋转过多且转身过快**，我们就会对其进行惩罚。
- *达到目标奖励*：我们对**达到目标的**Huggy进行奖励。

## 模型

整个训练是由一条命令完成的（并没有看到和讲解内部的代码！）

```bash
mlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=“Huggy” --no-graphics
```

定义了四个参数：

1. `mlagents-learn <config>`：超参数配置文件所在的路径。
2. `--env`：环境可执行文件所在的位置。
3. `--run-id`：要为训练运行 ID 指定的名称。
4. `--no-graphics`：在训练期间不启动可视化。

就是使用 `ML-Agents` 工具包，使用PPO模型，在 `Huggy` 环境中训练，具体的模型参数为：

```yaml
behaviors:
  Huggy:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    checkpoint_interval: 200000
    keep_checkpoints: 15
    max_steps: 2e6
    time_horizon: 1000
    summary_freq: 50000
```

一共要训练 2e6 步，在 google colab 上大约花费 40 min。

## 上传模型

> 如何在笔记本中登录 Hugging Face 可以查看[之前的文章](https://qmmms.github.io/posts/%E8%AE%AD%E7%BB%83%E6%8E%A2%E6%B5%8B%E5%99%A8%E6%AD%A3%E7%A1%AE%E9%99%8D%E8%90%BD%E5%9C%A8%E6%9C%88%E7%90%83%E4%B8%8A/#%E4%B8%8A%E4%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%B0-hub)

登录之后运行命令

```bash
mlagents-push-to-hf --run-id="HuggyTraining" --local-dir="./results/Huggy" --repo-id="QMMMS/ppo-Huggy" --commit-message="Huggy"
```

定义了 4 个参数：

1. `--run-id`：训练运行 ID 的名称。
2. `--local-dir`：代理被保存的地方，它是结果/<run_id名称>，所以在我的情况下是结果/第一次训练。
3. `--repo-id`：要创建或更新的 Hugging Face 存储库的名称。它始终是<您的 huggingface 用户名>/<存储库名称> 如果存储库不存在，**它将自动创建**
4. `--commit-message`：由于 HF 存储库是 git 存储库，因此您需要给出提交消息。

随后能看到提示信息：[INFO] Your model is pushed to the hub. You can view your model here: https://huggingface.co/QMMMS/ppo-Huggy

## 看看成果

可以在浏览器中和训练好的 Huggy 玩游戏！

- 在浏览器中打开 Huggy 游戏：https://huggingface.co/spaces/ThomasSimonini/Huggy
- 点击“玩我的 Huggy 模型”
- 选择您的模型存储库，即模型 ID（在我的例子中为 QMMMS/ppo-Huggy）。
- **选择要重播的模型**：我有多个模型，因为我们每 500000 个时间步长保存一个模型。但是由于我想要最新的，所以我选择`Huggy.onnx`

可以**尝试使用不同的模型步骤来查看代理的改进。**
