---
title: DualVGR： A Dual-Visual Graph Reasoning Unit
date: 2024-02-05 8:21:00 +0800

img_path: "/assets/img/posts/2024-02-05-DualVGR A Dual-Visual Graph Reasoning Unit"
categories: [深度学习]
tags: [读论文,视频问答]
math: true
---

> 读论文时间！
>
> 官方代码：https://github.com/MM-IR/DualVGR-VideoQA
>
> 前置知识：[ResNet](https://qmmms.github.io/posts/Deep-Residual-Learning-for-Image-Recognition/)、[ResNeXt](https://qmmms.github.io/posts/Aggregated-Residual-Transformations-for-Deep-Neural-Networks/)、[GNN](https://qmmms.github.io/posts/A-Brief-Introduction-to-Graph-Neural-Networks/)、[GCN](https://qmmms.github.io/posts/Semi-Supervised-Classification-With-Graph-Convolutional-Networks/)、[GAT](https://qmmms.github.io/posts/Graph-Attention-Networks/)、[LSTM](https://qmmms.github.io/posts/RNN%E4%B8%8ENLP%E5%9F%BA%E7%A1%80/#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clstm)、[BiLSTM](https://qmmms.github.io/posts/RNN%E4%B8%8ENLP%E5%9F%BA%E7%A1%80/#%E5%8F%8C%E5%90%91%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cbilstm)、[GloVe](https://qmmms.github.io/posts/Global-Vectors-for-Word-Representation/)
{: .prompt-info }

## 介绍

图像问答和视频问答之间存在两个区别：

1. 除了外观信息外，视频问答还需要了解运动信息以回答问题。
2. 视频问答需要在物体上执行时空推理，而图像问答只需要在物体上执行空间推理。

因此，现有的基于图的方法在多步推理方面表现不佳，忽视了视频问答（VideoQA）的两个属性：

1. 即使对于同一段视频，不同的问题可能需要不同数量的视频剪辑或对象来通过关系推理得出答案；
2. 在推理过程中，外观特征和运动特征之间存在复杂的相互依赖关系，它们相关且互补。

我们提出了一种**端到端视频推理单元（DualVGR）**，它是一种多模态推理单元，能够表示问题与视频剪辑之间丰富的交互。包含两个组件：

首先，我们设计了**查询惩罚模块**来生成查询引导的掩码，使有限数量与问题相关视频特征进入关系推理。该模块可以通过多次推理循环过滤掉不相关的视觉特征。

然后，为了充分捕获多视图视觉关系信息，我们提出了基于视频的**多视图图注意力网络**来揭示外观通道和运动通道之间的关系。所提出的图网络包括每个视觉通道的两个图。第一个图旨在学习每个特定视觉空间内的潜在互补关系，而另一个则是为了学习外观特征和运动特征之间的协同和相关关系。

基于视频的多视图图网络的损失包括一致性约束损失和差异约束损失。一致性约束损失是为了增强外观特征和运动特征之间的共性，而差异约束损失是为了增强它们之间的异质性。

我们**通过迭代的方式将我们的DualVGR模块堆叠到一个完整的DualVGR网络中**。通过多步推理，我们方法在几个主流数据集上达到了最先进的或竞争性的结果：MSVD-QA、MSRVTT-QA、SVQA。

## 相关工作

关于**图像问答**，有三条研究路线。

1. 第一条路线基于注意力机制定位图像中与问题最相关的视觉区域，然后投射视觉特征和文本通过单步将所有特征合并到一个公共潜在空间中。
2. 第二条路线侧重于通过递归单元对多步交互过程进行建模。同时，一些方法旨在在每个步骤中整合关系推理，特别是基于图形的方法。还有通过图注意力机制学习问题自适应的关系表示来建模多类型的物体间关系。
3. 第三种方法是神经符号推理。这种方法将整个任务分解为若干子任务，并设计了几个神经模块网络 (NMN)来解决这些子任务。

基于推理的方法可以分为四类：

1. 实现了空间和时间注意力机制，来迭代选择有用的信息以回答问题。
2. 专注于记忆网络，这在文本问答中很流行。然而，这些方法在执行多步推理时忽略了视觉关系信息。
3. 旨在通过简单的模块（如关系网络）进行关系推理。但是，该模块只能对有限数量的对象建模。
4. 旨在利用图神经网络GNN将关系信息整合到他们的框架中，考虑到GNN在关系建模方面强大的表示能力。

## 模型

### 定义

视频问答的任务可以描述如下。给定一个视频V和对应的问句q，代理程序旨在正确地推断出答案a。形式上，预测的结果 $\tilde{a}$由分类分数决定，其中θ是可训练模型参数：

$$
\tilde{a} = \mathop{\text{argmax}}_{a \in A} P_{\theta}(a|q,V)
$$

下图是DualVGR框架。首先，每个视频被划分为多个片段，并为每个片段提取外观特征和运动特征。同时，相应的问题嵌入双向 LSTM 中。其次，外观特征、运动特征和相应问题的特征被迭代地输入到我们的堆叠 DualVGR 单元中。

![](dualvgr.png)

我们使用每个 DualVGR 单元来确定视频中基于问题指导的注意力，并通过堆叠DualVGR单元来建模关系推理。再次，我们使用多模式分解双线性池化（MFB）融合每个剪辑的外观和运动特征，并利用注意机制融合所有剪辑的特征以获得最终的视觉矢量。最后，我们将视觉矢量与我们的问题矢量连接起来以预测答案。

### 视觉特征表示

我们首先将整个视频 V 中的 L 帧划分为 N 个等长片段，其中，每个视频剪辑$C_i$包含F帧，$F = \lfloor L / N \rfloor$：

$$
C = (C_1,...,C_N)
$$

我们从每个视频剪辑中提取两种信息源，即外观特征和运动特征。 每个视频剪辑$C_i$的外观特征表示如下，下标a表示外观：

$$
V_a^i=\{v_{a,1}^i,...,v_{a,F}^i\} \in \mathbb{R}^{2048\times F}
$$

类似的，每段的运动特征由 $V_m^i \in \mathbb{R}^{2048 \times 1}$ 表示，下标m表示运动。

为了与最先进的方法进行公平比较，外观特征是ResNet101的pool5输出，而运动特征是从ResNeXt-101中提取的。

### 语言特征表示

我们将每个问题中的单词嵌入到固定长度的向量中，初始化为 300 维 GloVe 向量，其中 $L_q$ 是每个问题的长度：

$$
W=\{w_i: 1 \leq i \leq L_q , w_i \in \mathbb{R}^{300×1}\}
$$

然后，我们将这些词嵌入传递给 BiLSTM 网络以获得上下文感知的嵌入向量。

与仅使用 LSTM 的最后一个隐藏状态，而不进行深入分析来表示问题的不同方法相比，我们在每个步骤中处理 BiLSTM 生成的所有隐藏状态序列 $Q\in \mathbb{R}^{d×L_q}$。

此外，为了进一步执行基于图的推理，外观特征应具有与运动特征相同的维数。因此，我们进一步将每个视频剪辑的外观特征 $V^i_a$ 传递给一个 BiLSTM 网络生成 $V^i_a \in \mathbb{R}^{d×1}$，并将每个视频剪辑的运动特征投影到固定大小的矢量上 $V^i_m \in \mathbb{R}^{d×1}$，维度为 d：

$$
\left\{
\begin{array}{l}
\{ V^i_a,O\} = \text{BiLSTM}(V^i_a, \theta^{(v)}_\text{BiLSTM}), \\
\{ E^Q,Q\} = \text{BiLSTM}(W, \theta^{(q)}_\text{BiLSTM})
\end{array}
\right.
$$

其中，$O∈ \mathbb{R}^{d×L_q}$ 是基于外观的 BiLSTM 的隐藏状态，$E^Q∈\mathbb{R}^{d×1}$ 是最后一个隐藏层的输出，表示问题的全局特征。

### DualVGR单元

为了回答给定视频中的问题，模型需要具备定位与问题相关的视频剪辑、使用外观和运动通道理解视频以及执行多步推理的能力。

DualVGR 单元能确定与问题相关的视频剪辑，挖掘外观和运动之间的内部或相互关系，并将补充的关系特征与相应的视觉特征相结合，然后对 DualVGR 单元进行堆叠以进行多步关系推理。

![](du.png)

如上图所示，它由两个模块组成：查询惩罚模块和基于视频的多视角图注意力网络。查询惩罚模块通过在视频特征上应用基于查询的掩码来过滤掉不相关的视频剪辑。基于视频的多视角图注意力网络通过构建外观特定和运动特定的图来揭示外观和运动之间的潜在互补关系，并通过构建外观-运动相关性和运动-外观相关性图来揭示外观和运动之间的相关关系。

### 查询惩罚模块

由于仅需要与查询相关的信息来推断正确答案，此外，当执行多步推理时，尤其是长且复杂的问句，人们往往会在意问题的不同文本部分。因此，我们提出一个查询惩罚模块来模仿人类的逐步推理行为，并在每个推理步骤中过滤掉不相关的视觉特征。

基于上下文词嵌入 $Q = (Q^1,..., Q^{L_q})$ 和初始词嵌入 $W = (w^1,..., w^{L_q})$，我们使用自注意力机制获得当前步骤的**问题特征** $q_w^{(t)}$ ，其中 $W_1 \in \mathbb{R}^{d\times d},W_2 \in \mathbb{R}^{d\times 1}$ 是可学习的参数：

$$
\left\{
\begin{array}{l}
w^i_q=\text{L2Norm}(W_1Q^i) \\
\alpha^i_q=\text{softmax}(W_2w^i_q) \\
q_w^{(t)}=\sum^{L_q}_{i=1} \alpha^i_q w^i
\end{array}
\right.
$$

在第t次迭代中获得问题表示 $q_w^{t}$ 后，我们使用它来过滤掉与查询不相关的视觉特征。具体来说，我们首先通过全连接网络将问题转换为视觉空间，然后使用点积操作来建模当前问题与视觉剪辑特征的相关性。通过门控机制，我们可以为每个剪辑特征获得查询引导的掩码值。形式化地，惩罚机制由以下公式给出，其中$W_a^{(t)} \in \mathbb{R}^{300\times d}$ 是可学习的参数：

$$
\left\{
\begin{array}{l}
\beta^{(t)}_{a,i}=\tilde{V}_a^i W_a^{(t)} q_w^{(t)} \\
\beta^{(t)}_{a}=\sigma([\beta^{(t)}_{a,1},...\beta^{(t)}_{a,N}]) \\
\tilde{E}^{(t)}_a = [\beta^{(t)}_{a,1}\tilde{V}_a^1,...\beta^{(t)}_{a,N}\tilde{V}_a^N]
\end{array}
\right.
$$

类似的，动作相关使用相同公式，只是把下标a 换成m。

总结，通过与掩码相乘以及相应的基于剪辑的外观特征和运动特征，我们在每个步骤中过滤掉当前不相关的视觉信息。

### 基于视频的多视角图注意力网络

如前所述，外观和运动通道对于视频理解都至关重要。为了充分揭示这两个通道中的补充信息，我们不仅需要从视频剪辑中提取外观和运动特征，还需要考虑每个通道内视频剪辑之间的关系以及每个视频剪辑两个通道之间的关系。为此，我们的工作试图将通道内的关系和通道间的联系合并到外观和运动特征中。

受GAT的启发，它使用自注意机制在其邻居上更新节点表示，并且能够处理归纳学习问题，特别是任意结构的图学习问题，我们构建了外观图，其中节点为每个剪辑的外观特征，构建了运动图，其中节点为每个剪辑的运动特征，然后遵循GAT提出的方法在这些图中将邻接关系编码为节点表示。

通过这种方式，可以将通道内的关系分别聚合到外观和运动特征中。对于通道间的关系，我们遵循AM-GCN的思想，这是一种新的用于图形分类任务的GCN，可以通过自适应地融合来自节点功能、拓扑结构的具体和通用嵌入来优化整合节点功能和拓扑结构。每个通道的特定嵌入保持该通道的特性，这与其他通道的嵌入应有所不同。两个公共嵌入模型来自两个通道的相关信息，因此它们应该是相互一致的。

> 没有使用简单的GNN，简单GNN的拓扑结构和节点特征之间提取深度相关信息的能力远非最佳。

在外观通道中，我们构建了两个无向完整图网络，包括外观独立图（AIG）和运动外观相关图（AMC），每个查询惩罚剪辑基于外观特征作为节点。AIG在外观通道内学习特定的空间时间上下文嵌入 $Z^{(t)}_A$ 。

AMC旨在提取与运动通道共享的外观和运动通道的相关关系特征 $Z^{(t)}_{CA}$。

对于运动通道，我们也使用两个图网络来学习上下文嵌入，包括运动独立图（MIG）和运动外观相关图（MAC）。它们的目标与外观通道中的图形相同。根据这两个运动图，我们可以得到具体的嵌入 $Z^{(t)}_M$ 

和相关的嵌入 $Z^{(t)}_{CM}$。

 每个图都实现了一个多头图注意力网络（GAT），用于建模剪辑特征之间的关系。具体来说，每个头部α 两个节点之间的注意力分数计算如下：

$$
\left\{
\begin{array}{l}
h^t_i=U_t \tilde{E}^{(t)}_i+b_t \\

\alpha_{ij}^{(t)}=\frac{\exp ( \text{LeakyReLU}(W_t[ h_i^t \big| \big| h_j^t ]) )}{\sum_{k\in {N}_i}{\exp( \text{LeakyReLU}(W_t[ h_i^t \big| \big| h_k^t ])) )}}
\end{array}
\right.
$$

其中，$U_t ∈ \mathbb{R}^{d×d_1}$ 和 $W_t ∈ \mathbb{R}^{2d_1×1}$ 是可学习参数，
$$
||
$$ 
是连接操作，$N_i$ 是图中节点 i 的一阶近邻。

更新节点公式，$α^k_{i,j}$ 是来自第 k 个注意力机制的归一化注意力系数。每个节点特征的最终形状为 $\mathbb{R}^{ Kd_1×1}$：

$$
\tilde{h}_i^{t}= \overset{K}{\underset{k=1}{\big| \big|}} \sigma\left( \sum_{j\in {N}_i}{\alpha_{ij}^{k,t} }h_j \right)
$$

最后，我们提出了两个损失函数来增强我们的图的多视图表示能力：一致性约束和差异性约束。一致性约束是为了增强外观空间和运动空间相关上下文信息之间的共性。差异性约束是为了增强特定嵌入和相关嵌入之间的独立性。

### 一致性约束

对于两个输出嵌入向量 $Z_{CA}$ 和$Z_{CM}$，我们为我们的多视图任务设计了一个一致性约束，它可以增强它们之间的共性。

首先，我们将最终嵌入矩阵 
$$Z^{(t)}_{CA} 和 Z^{(t)}_{CM}$$ 
分别归一化为 
$$Z^{(t)}_{CAnor} 和 Z^{(t)}_{CMnor}$$​ 。
N 个嵌入的相似性可以计算为：

$$
S_A^{(t)} = Z_{CAnor} \cdot Z_{CAnor}^T
$$

$$
S_M^{(t)} = Z_{CMnor} \cdot Z_{CMnor}^T
$$

一致性表示两个相似度矩阵应该尽可能相似。单个单元的一致性损失可以表示为：

$$
\mathcal{L}_c^{(t)} = ||S_A^{(t)}-S_M^{(t)}||_2
$$
然后整个网络的一致性损失如下，T为迭代次数：

$$
\mathcal{L}_c = \frac{1}{T}\sum_T^{t=1}\mathcal{L}_c^{(t)}
$$

### 差异约束

由于特定嵌入和通用嵌入（如 $Z_A$ 和 $Z_{CA}$）是从具有相同拓扑结构的图中学习的，为了确保这两种嵌入可以捕获不同的信息，我们使用 HSIC 来增强这两种嵌入之间的独立性，其中K是Gram 矩阵。

$$
HSIC^{(t)}(Z_A,Z_{CA})=(n-1)^{-2}tr(RK_ARK_{CA})
$$

动作相关的公式类似。最后，整个网络的差异约束由以下公式给出：

$$
\mathcal{L}_d^{(t)}=HSIC^{(t)}(Z_A,Z_{CA})+HSIC^{(t)}(Z_M,Z_{CM})
$$

$$
\mathcal{L}_d = \frac{1}{T}\sum_T^{t=1}\mathcal{L}_d^{(t)}
$$

### 注意力

现在我们在外观空间有嵌入 $Z_A$ 和 $Z_{CA}$，在运动空间有两个嵌入 $Z_M$ 和 $Z_{CM}$。为了自适应地捕获每个空间中的上下文信息，我们使用注意力机制来融合外观空间中两个嵌入以及在运动空间中的两个嵌入。它们对应的注意力重要性分数由下式给出：

$$
(\alpha_a,\alpha_{ca}) = Attn(Z_{A},Z_{CA})
$$
动作相关公式类似。

具体来说，计算注意力和残差连接的公式如下：

![](dualattf.png)

最后得到更新后的基于剪辑的特征 $\tilde{V}$

### 多步推理

最后，DualVGR 单元被堆叠为一个链来执行最终的 DualVGR 网络：

$$
\{\tilde{V}^i_a,\tilde{V}^i_m\}=DualVGR(\tilde{V}^i_a;\tilde{V}^i_m;Q;W)
$$

通过多次迭代，我们的双视图生成器的最终视觉表示包含关于与问题相关的剪辑的互补信息，包括外观、运动以及它们之间的对应关系。

### 视频表示融合

在最后一步，我们得到每个剪辑的外观特征和运动特征，通过使用多模态因子分解双线性池化（MFB）将其融合以获得每个剪辑的最终视觉表示 $V^i_f$。使用称为图读取操作（GRO) 的注意力机制来融合剪辑特征。

$$
V^i_f=MFB(\tilde{V}^i_a,\tilde{V}^i_m)
$$

$$
\tilde{o}=Attn(V^i_f)
$$

### 答案解码器

![](dualde.png)

### 总损失

我们将开放式的视频问答任务定义为分类问题。因此，我们使用交叉熵损失函数 $L_t$来处理该任务。然后，总损失为

$$
\mathcal{L}=\mathcal{L}_t+\gamma\mathcal{L}_c+ \beta \mathcal{L}_d
$$

## 实验

### 数据集

MSVD-QA：从微软研究院视频描述 (MSVD) 数据库中收集了 1970 段修剪过的视频。该数据库包含 50,500 对由 NLP 算法自动生成的问题-答案对，其中包括五种一般类型的问题：what、how、when、where 和 who。平均视频长度约为 10 秒，平均问题长度约为 6 个单词。因此，这是一个小规模数据集，包含简短的问题。我们在该数据集上进行实验，以测试我们的模型在真实世界中的短视频上的泛化能力。

MSRVTT-QA ：与 MSVD-QA 相比，MSR-VTT 包含更长的视频和更复杂的场景。我们在该数据集上进行实验以测试我们的模型对于长视频的真实数据集的性能。包含 10,000来自 MSR-VTT 数据集的剪辑视频，以及通过自然语言处理算法生成的 243,000 对问答对。平均视频长度约为 15 秒， 平均问题长度为约 7 个单词。

SVQA：这是一个包含 12,000 个合成视频的大规模合成数据集，其中大约有 120k 个问答对。具体来说，视频是由 Unity3D 生成的，每个视频的长度均为 10 秒。同时，问答对由问题模板自动生成，这些问题平均长度为 20 词。此外，每个问题可以很容易地分解为人类可读的逻辑链或树状结构。该数据集的目标是测试视频问答系统的推理能力。

### 实施细节

对于MSVD-QA中的每个视频，我们将视频划分为8个片段。对于MSRVTT-QA，每个视频被分成16个片段。此外，在SVQA数据集中，每个视频被分割为20个片段。分段的数量由{4、8、12、16、20、24}的网格搜索方法确定。

对于所有数据集，默认情况下，每个剪辑包含16帧。视频外观编码器和问题编码器都是单层双向 LSTM。维度d = 768，d1 = 192，多头数量k = 4。

迭代步长t对MSVD-QA设置为1，对MSRVTT-QA设置为6，对SVQA设置为4。

对于损失函数，我们使用网格搜索方法选择最佳系数。具体来说，γ值的集合为{0、1e-4、1e-3、1e-2、1e-1、1、10、100、1000}，而剩余系数β的值集合为{0、1e-8、1e-6、1e-4、1e-2、1、10、100}。经过搜索，我们获得了所有数据集的最佳系数：

- γ = 100，β = 1e-6在MSVD-QA中。
- γ = 100，β = 1e-6在MSRVTT-QA中。
- γ = 1，β = 1e-6在SVQA中。

我们的框架是在PyTorch中实现的，网络是通过带有固定学习率1e-4的Adam优化器进行训练的。批量大小设置为256。所有的实验都是在两个NVIDIA RTX 2080Ti GPU上运行的。

效果如下

![](dualsota.png)

### 对迭代次数T的分析

MSVD-QA 数据集中的问题通常很短，这意味着代理不需要执行复杂的关联推理就可以完全理解问题。因此，在进行更多步关联推理时，整体准确率会降低。

当推理迭代次数从1增加到6时，测试准确率在MSRVTT-QA数据集上从35.29％提高到35.52％。由于MSRVTT-QA数据集比MSVD-QA具有更复杂的问题和更长的视频，因此它可以受益于更多的推理迭代次数超过我们的DualVGR单元。然后，当迭代步骤设置为7时，模型的性能变得非常稳定。

SVQA 是一个用于测试模型关联推理能力的数据集。因此，问题是复合且复杂的，需要代理执行多步关联推理。结果表明，SVQA数据集的准确性也受到更高迭代步骤数的影响。该网络在SVQA上的总体测试精度比四步网络高1.71%。然后，当步长增加 5 时，模型的表现非常稳定。

### 对剪辑个数的分析

由于SVQA中的问题具有复合性，需要代理理解整个视频的内容以及与视频中相关对象之间的时空关系，因此视觉事实的关键信息可能均匀分布在视频中。因此，我们还对SVQA数据集中的片段分割数量进行了详细的分析。

分别包含16个和20个片段的网络在SVQA测试集上比包含4个片段的网络的整体准确率分别提高了+0.84% 和 +0.93%。这表明空间时间关系建模通过图神经网络可以受益于更多的分割片段。随着更多片段的分割，视频中的关键信息会更均匀地分布。最后，由于 SVQA 中的每个视频都具有相同的长度为 300 帧，因此包含 24 个片段的网络中的每个片段表示都会包含大量噪声。因此，随着数量 N 的增加，它们的表现会下降。

### 查询惩罚模块的贡献

为了更好地理解在DualVGR中的查询惩罚模块的贡献，我们在下图中提供了MSVD-QA和SVQA数据集上查询特征注意力结果和查询引导掩码值的可视化示例。

在真实数据集MSVD-QA的一个步骤的第一个例子中，这个问题想要问的是*谁在一个盒子里展示枪？*我们的模型主要关注问题的右边“展示枪”。这个问题要求代理人关注运动信息来推断答案。

![](duale1.png)

我们可以从这个例子中观察到，在所有剪辑中，基于运动的掩码值都高于基于外观的掩码值，这证明了我们查询惩罚模块在真实数据集上的有效性。此外，前两个剪辑的基于运动的掩码值比其他剪辑高，进一步证明了我们的模型更加关注相关视频剪辑以预测答案的正确性。

对于SVQA数据集的第二个例子，这段视频非常简单，因为它只包含四个对象。灰色立方体一直在旋转。然后，青色物体开始旋转一段时间。与此同时，绿色圆柱体开始向上移动。在迭代步骤t = 1时，问题更加关注“旋转现在”，因此基于运动的掩码值高于基于外观的掩码值。

![](duale2.png)

在迭代步骤t = 2时，如果模型对“青色对象”给予了更多关注，那么基于外观的掩码值就会高于基于运动的掩码值，从而学习到青色对象的上下文表示。

最后，在迭代步骤 t=3 时，模型会同时考虑这两种情况来推断最终答案。掩码值比之前的步骤更接近彼此。通过多步消息传递，我们的 DualVGR 进一步发现了更多与问题相关的视觉语义，以推断答案。可视化结果进一步验证了我们设计的有效性。
