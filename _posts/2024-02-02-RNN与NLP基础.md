---
title: RNN与NLP基础
date: 2024-02-02 18:21:00 +0800

media_subpath: "/assets/img/posts/2024-02-02-RNN与NLP基础"
categories: [深度学习]
math: true
---

自然语言处理研究实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理技术发展经历了**基于规则的方法**、**基于统计学习的方法**和**基于深度学习的方法**三个阶段。自然语言处理由浅入深的四个层面分别是**形式、语义、推理**和**语用**，当前正处于由语义向推理的发展阶段。

![](nlp.png)

## 词向量学习模型

是一种将自然语言中的字词转换为计算机可以理解的稠密向量的方法，基本思想就是用词来预测词。其中包含两种算法：CBOW和Skip-gram。CBOW通过上下文来预测当前词，Skip-gram通过当前词预测上下文。

### CBOW

CBOW模型的输入是某一个特征词的上下文相关的词对应的词向量，而输出是这个特定词的词向量。计算过程如下：

1. 输入特征词上下文相关词的one-hot编码，维度为1\*V，其中V是词空间的大小。
2. 这些词的词向量和共享矩阵W相乘。W的维度为V\*N，其中N是自己定义的维度，也是最终word embedding的维度。每个上下文词都会计算得到一个1\*N向量。
3. 将这些向量相加取平均，得到维度为1\*N的隐藏层向量。
4. 将该向量与$W'_{N×V}$相乘，得到输出层的维度为1\*V。
5. 将输出层的向量经过softmax归一化处理得到新的1*V向量，取概率最大的值对应的位置为预测结果。
6. 将预测的中心词和真实结果计算误差，一般使用交叉熵函数计算误差。
7. 根据误差进行反向传播。不断更新W和$W'_{N×V}$的值。
8. 最终得到W矩阵，任何一个单词的one-hot编码乘这个矩阵都将得到自己的词向量。

![](wordtovec.png)

### Skip-Gram

Skip-Gram模型的输入是特定词的词向量，输出是特定词对应的上下文词向量。计算过程如下：

- 输入特定词的one-hot编码，维度为1\*V，其中V是词空间的大小。
- 特定词的词向量和共享矩阵W相乘。W的维度为V\*N，其中N是自己定义的维度，也是最终word embedding的维度。得到一个1\*N的隐藏层向量。
- 隐藏层向量和维度为N\*V的矩阵相乘，得到1\*V的输出向量。
- 将该输出向量经过softmax，选取概率最大的位置为预测结果。
- Skip-gram通过反向传播算法和梯度下降来学习权重，训练数据的输入一般是一个词$W_I$，上下文窗口为C，它的输出是$W_I$的上下文$W_O$，损失函数是输出词的条件概率：

  $$E=-\log p(W_{O,1},W_{O,2},W_{O,3}...W_{O,C}|W_I)$$

- 训练完成后，得到权重矩阵W。

### 层次化softmax方法

CBOW/skip-gram模型的输出层使用的softmax函数，计算代价很大，对大规模的训练语料来说，训练非常耗时。

层次化softmax是一种对输出层进行优化的策略。输出层从原始模型单层计算概率值改为利用Huffman树计算概率值。

![](cch.png)

### 负采样方法

与层次化softmax方法相比，负采样不再使用复杂的Huffman树，而是采用随机负采样策略，优化目标改为：最大化正样本的概率，同时最小化负样本的概率。

![](fcy.png)

对于给定上下文，词*w*就是一个正样本，其他词就是负样本。但是负例样本太多了，解决方法是：在语料库中，各个词出现的频率是不一样的，我们采样的时候要求高频词选中的概率较大，而低频词选中的概率较小。这就是一个带权采样的问题。

## 句子嵌入表示模型

### Bag-of-words

仅仅求和，得到的句子向量是定长的，句子向量的长度等于词向量的长度。但是丢失了词汇之间的顺序信息。

![](pl.png)

### Pooling

句子向量长度是词向量的3倍，除了最大、最小和平均，当然也可以追加其他操作继续进行扩展。这里+的符号意思是拼接

### CNN

卷积神经网络的思想，分别卷积了大小为2和3的窗口，使用最大池化，是一个有监督学习模型，可以学习到词序信息

![](cnn.png)

### Variations

层级CNN，在输入端引入了字符向量。在文本分析的很多任务中，都证明引入字符向量可以提高分析性能

## 文档嵌入表示模型

![](do.png)

![](do2.png)

## 循环神经网络RNN

### 简单RNN

![](srnn.png)

$$
h_t=f(Uh_{t-1}  + W_{x_t}+ b)
$$

$$
o_t=y_t=softmax(Vh_t)
$$

- *f* 是非线性函数，通常为 *sigmod*函数或 *tanh* 函数
- ht是在时刻t的隐藏状态，是网络的memory
- yt是在时刻t的输出
- RNN在所有步中采用共同的参数(U,V,W)，表示在每一步执行相同的任务，仅仅是输入不同而已

如果我们把$o_t$和$h_t$认为是一个东西，图可以这么画：

![](srnn2.png)

![](srnn3.png)

RNN模型有以下三个特点：

- 时序性、递归性：一般输入来自两个方面，一个是之前状态 ht-1，和当前状态的输入xt 。
- 参数共享：每一步的参数矩阵都是共享的，主要的参数矩阵也是上述的两个方面，W 和 U 。
- cell的设计：为了解决RNN更新时指数式的梯度弥散，梯度爆炸的问题和控制 cell 保留信息比例的问题，设计了GRU，LSTM cell，具体见后面。

更新参数：

$$
\begin{align}
E_t(y_t,\hat{y_t})=-y_tlog\hat{y_t} \\
E(y,\hat{y})=\sum_tE_t(y_t,\hat{y_t}) \\
\nabla U = \frac{\partial E}{\partial U}\\
\nabla V = \frac{\partial E}{\partial V}\\
\nabla W = \frac{\partial E}{\partial W}
\end{align}
$$

如果最大奇异值大于1，则梯度将爆炸，称为爆炸梯度。如果最大奇异值小于1，则梯度将消失，称为消失梯度。

权重在所有层中共享，导致梯度爆炸或消失。除此之外，很久以前的输入，对当前时刻的网络影响较小；反向传播的梯度，也很难影响很久以前的输入。

解决方法：梯度爆炸：梯度剪裁。梯度消失：LSTM、GRU。

### 长短时记忆神经网络LSTM

长短时记忆神经网络（Long Short-Term Memory Neural Network，LSTM）是循环神经网络的一个变体，可以有效地解决长期依赖问题/梯度消失问题。

细胞的状态在整条链上运行，只有一些小的线性操作作用其上，信息很容易保持不变的流过整条链。

**门**(Gate)是一种可选地让信息通过的方式，由一个Sigmoid神经网络层和一个点乘法运算组成。Sigmoid神经网络层输出0和1之间的数字，这个数字描述每个组件有多少信息可以通过， 0表示不通过任何信息，1表示全部通过	

- **遗忘门**决定我们要从细胞状态中丢弃什么信息。它查看ht-1(前一个隐藏状态)和xt(当前输入)，并为状态Ct-1(上一个状态)中的每个数字输出0和1之间的数字，1代表完全保留，而0代表彻底删除
- **输入门**决定要在细胞状态中存储什么信息。首先，输入门的Sigmoid层决定了我们将更新哪些值。然后，一个tanh层创建候选向量C ̃_t,该向量将会被加到细胞的状态中。最后，结合这两个向量来创建更新值。
- **更新记忆**：将上一个状态值$C_{t-1}$乘以f_t，以此表达期待忘记的部分。之后将得到的值加上 i_t∗C ̃_t。这个得到的是新的状态值
- **输出门**决定我们要输出什么， 此输出将基于当前的细胞状态。首先，通过一个sigmoid层，决定了我们要输出细胞状态的哪些部分。然后，将细胞状态通过tanh（将值规范化到-1和1之间），并将其乘以Sigmoid门的输出，至此完成了输出门决定的那些部分信息的输出。

![](lstm.png)

### 双向长短期记忆神经网络BiLSTM

首先看2层的 LSTM 是如何运转的

![](lstm2.jpg)

 再看1层的 BiLSTM 是如何运转的

![](bilstm.png)

用一个文本情感分析的例子来说明：单层的BiLSTM其实就是2个LSTM，一个正向去处理序列，一个反向去处理序列，处理完后，**两个LSTM的输出会拼接起来**。

特别注意：在这个案例中，所有时间步计算完后，才算是BiLSTM的结果，正向LSTM经过6个时间步得到一个结果向量，反向LSTM同样经过6个时间步后，得到另一个结果，然后这两个结果向量拼接起来，作为BiLSTM的最终输出。

> 疑问：是不是NLP里面大多时候只需要最后一个时刻的输出即可？
>
> 答案：这属于N VS 1结构，即N次输入，1次输出。这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。

再看看2层的 BiLSTM 是如何运转的

![](bilstm2.png)

### 门限循环单元GRU

门限循环单元是一种比 LSTM 更加简化的版本。在 LSTM 中，输入门和遗忘门是互补关系，因为同时用两个门比较冗余。GRU 将输入门与和遗忘门合并成一个门：更新门（Update Gate），同时还合并了记忆单元和隐藏神经元。GRU没有单独的细胞状态。

![](gru.png)

![](gru2.png)

### 堆叠(Stack)循环神经网络

![](sta.png)

一方面来说，如果把循环网络按时间展开，不同时刻的状态之间存在非线性连接，循环网络已经是一个非常深的网络了。

另一方面来说，这个网络是非常浅的。隐藏状态到输出，以及输入到隐藏状态之间之间的转换只有一个非线性函数。

### 双向循环神经网络

![](de.png)

在Forward层从0时刻到i时刻正向计算一遍，得到并保存每个时刻向前隐含层的输出；

在Backward层从i时刻到0时刻反向计算一遍，得到并保存每个时刻向后隐含层的输出；

最后在每个时刻结合Forward层和Backward层的相应时刻输出的结果得到最终的输出。

### 应用

![](yy.png)

## 注意力机制

![](att1.png)

![](att2.png)

![](att3.png)

![](att4.png)

![](att5.png)

注意力机制运用在机器翻译中，<源语言词，目标语言词>的注意力权重基本与两个词互为翻译的情况一致

注意力机制运用在机器阅读中，给文章中每句话一个attention权重，根据问题选出最有可能包含答案的句子