---
title: 深度学习中的优化算法
date: 2023-02-01 08:58:00 +0800
categories: [深度学习]
tags: [深度学习]

img_path: "/assets/img/posts/2023-01-30-深度学习中的优化算法"
math: true
---

> 这属于一个文章系列，前置知识移步：[深度学习文章分类](/categories/深度学习/)
{: .prompt-tip }

{% raw %}

## Mini-batch 梯度下降

-------

### 思想

之前学过，向量化能够有效地对所有$m$个样本进行计算，允许处理整个训练集。具体来说，$X$的维数是$(n_{x},m)$，$Y$的维数是$(1,m)$，向量化能够相对较快地处理所有$m$个样本。

但是，如果$m$很大的话，处理速度仍然缓慢。比如$m$是500万或5000万或者更大的一个数。

可以把训练集分割为小一点的子集训练，这些子集被取名为**mini-batch**，假设每一个子集中只有1000个样本，那么把其中的$x^{(1)}$到$x^{(1000)}$取出来，将其称为第一个子训练集（也叫做**mini-batch**），称为$$X^{ \{ 1 \} }$$，然后再取出接下来的1000个样本，从$x^{(1001)}$到$x^{(2000)}$，称为$$X^{\{2\}}$$，然后再取1000个样本，以此类推。

如果训练样本一共有500万个，每个**mini-batch**都有1000个样本，那么共有5000个**mini-batch**，最后得到是$$ X^{ \left \{ 5000 \right \}} $$，维数都是$(n_{x},1000)$。对$Y$也要进行相同拆分处理，一直到$$Y^{\{ 5000\}}$$，维数都是$(1,1000)$。

> 复习一下：
>
>-   上角小括号$(i)$表示训练集里的值，$x^{(i)}$是第$i$个训练样本。
>-   上角中括号$[l]$来表示神经网络的层数，$z^{\lbrack l\rbrack}$表示神经网络中第$l$层的$z$值。
>-   现在引入了大括号$$\{t\}$$来代表不同的**mini-batch**，有$$X^{\{ t\}}$$和$$Y^{\{ t\}}$$。

### 介绍

**batch**梯度下降法指的是之前讲过的梯度下降法算法，就是同时处理整个训练集。

相比之下，**mini-batch**梯度下降法，指的是每次同时处理的单个的**mini-batch** $X^{\{t\}}$和$Y^{\{ t\}}$，而不是同时处理全部的$X$和$Y$训练集。

### 处理流程

前向传播，具体看公式：

$$
Z^{\lbrack 1\rbrack} = W^{\lbrack 1\rbrack}X^{\{ t\}} + b^{\lbrack1\rbrack}
$$

$$
A^{[1]} =g^{[1]}(Z^{[1]})
$$

每层都以此类推，直到：

$$
A^{\lbrack L\rbrack} = g^{\left\lbrack L \right\rbrack}(Z^{\lbrack L\rbrack})
$$

计算损失成本函数$J$（子集规模是1000，加上正则化）：

$$
J^{\{t\}} = \frac{1}{1000}\sum_{i = 1}^{l}{L(\hat y^{(i)},y^{(i)})} +\frac{\lambda}{2\cdot 1000}\sum_{l}^{}{||W^{[l]}||}_{F}^{2}
$$

执行反向传播来计算$$J^{\{t\}}$$的梯度：

$$
W^{[l]}:= W^{[l]} - \alpha dW^{[l]}
$$

$$
b^{[l]}:= b^{[l]} - \alpha db^{[l]}
$$

至此，完成了“一代”（**1 epoch**）的训练。“一代”这个词意味着只是一次遍历了训练集。因为有5000个**mini-batch**，当全部遍历一遍训练集后，能做5000个梯度下降，在训练巨大的数据集时比较常用。

### 注意

![两种梯度下降法的区别](b5c07d7dec7e54bed73cdcd43e79452d.png)

使用**batch**梯度下降法时，每次迭代你都需要历遍整个训练集，如果成本函数$J$是迭代次数的一个函数，它应该会随着每次迭代而减少。

使用**mini-batch**梯度下降法，每次迭代下你都在训练不同的样本集或者说训练不同的**mini-batch**，如果要作出成本函数$J$的图，很可能会看到这样的结果，走向朝下，但有更多的噪声。

>   Q：**mini-batch**的大小应该怎么选择呢？
>
>   A：样本集较小就没必要使用**mini-batch**梯度下降法，这里的少是说小于2000个样本，这样比较适合使用**batch**梯度下降法。不然，样本数目较大的话，一般的**mini-batch**大小为64到512。（考虑到电脑内存设置和使用的方式，如果**mini-batch**大小是2的$n$次方，代码会运行地快一些）

## 动量梯度下降法

--------

### 思想

**Gradient descent with Momentum**，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法。基本的想法就是计算梯度的指数加权平均数。

举个例子，如果优化成本函数形状如图（蓝线），红点代表最小值的位置。如果按照传统的梯度下降算法，下降的过程将很曲折，或者说很慢。

![动量梯度下降法和标准的梯度下降算法比较](aauu.png)

注意到真正有用的部分是水平方向的下降，垂直方向的下降是不必要的，我们可以**平均**每次的偏移量，这样垂直方向的平均梯度就为0，只从水平方向梯度下降。如图红线。

### 指数加权平均数

对于一系列数据 $\theta_1,\theta_2,\theta_3 \dots \theta_n$，计算指数加权平均数 $v_{t}$ 的递推公式为：
$$
v_{t} = \beta v_{t - 1} + (1 - \beta)\theta_{t}
$$
其中 $\beta$ 是一个常数，最常用的值是0.9。$\beta$ 越大，曲线更平坦，原因在于你多平均了一些值。缺点是曲线进一步右移，因为现在平均的温度值更多，要平均更多的值，适应地更缓慢一些，所以会出现一定延迟。

举个例子，对于下面这张图，蓝点代表数据，红线代表的指数加权平均数选用的 $\beta$ 较小；绿线代表的指数加权平均数选用的 $\beta$ 较大。

![指数加权平均数](a3b26bbce9cd3d0decba5aa8b26af035.png)

>   **技巧：偏差修正**
>
>   偏差修正可以让平均数运算更加准确。
>
>   因为当 $v_{0}$ 初始化为 0 时，最初计算的指数加权平均数会偏小，在估测初期，可以使用公式：
>
>   $$
>   v_{t} = \frac{\beta v_{t - 1} + (1 - \beta)\theta_{t}}{1- \beta^{t}}
>   $$
>
>   通常，大家不在乎执行偏差修正，因为大部分人宁愿熬过初始时期，拿到具有偏差的估测，然后继续计算下去。如果你关心初始时期的偏差，在刚开始计算指数加权移动平均数的时候，偏差修正能帮助你在早期获取更好的估测。

### 处理流程

简单来说，就是使用梯度的平均值来代替梯度，具体公式为：

$$
v_{{dW}}= \beta v_{{dW}} + \left( 1 - \beta \right)dW
$$

$$
v_{db} = \beta v_{{db}} + ( 1 - \beta){db}
$$

$$
W:= W -av_{{dW}}
$$

$$
b:= b - a v_{db}
$$

## RMSprop算法

------

RMSprop算法的思想和动量梯度下降法相似，使用梯度的平均值来代替梯度，具体公式为：

$$
S_{dW}= {\beta} S_{dW} + (1 -{\beta}) {dW}^{2}
$$

$$
S_{db}= {\beta} S_{db} + (1 - {\beta}){db}^{2}
$$

$$
W:= W -\alpha \frac{dW}{\sqrt{S_{dW}}+\varepsilon}
$$

$$
b:=b -\alpha\frac{db}{\sqrt{S_{db}}+\varepsilon}
$$

其中，$\varepsilon$ 的加入只是为了避免分母过小，$10^{-8}$是个不错的选择。 $\beta$ 依旧是一个常数，这里最常用的值是0.999。

## Adam算法

------

Adam算法(Adam optimization algorithm)，基本上就是将**Momentum**和**RMSprop**结合在一起，思想也类似，使用梯度的平均值来代替梯度，具体流程如下 :

首先根据上面的公式计算出 $v_{{dW}}$, $v_{db}$, $S_{dW}$, $S_{db}$

计算偏差修正的值，因为**Momentum**和**RMSprop**都会使用$\beta$，所以区分${\beta}_1$和${\beta}_2$

$$
v_{dW}^{\text{corrected}}= \frac{v_{dW}}{1 - \beta_{1}^{t}},v_{db}^{\text{corrected}} =\frac{v_{db}}{1 -\beta_{1}^{t}}
$$

$$
S_{dW}^{\text{corrected}} =\frac{S_{dW}}{1 - \beta_{2}^{t}},S_{db}^{\text{corrected}} =\frac{S_{db}}{1 - \beta_{2}^{t}}
$$

最后更新权重

$$
W:= W - \frac{a v_{dW}^{\text{corrected}}}{\sqrt{S_{dW}^{\text{corrected}}} +\varepsilon}
$$

$$
b:=b - \frac{\alpha v_{\text{db}}^{\text{corrected}}}{\sqrt{S_{\text{db}}^{\text{corrected}}} +\varepsilon}
$$

-   其中，Momentum使用的${\beta}_1$常用的缺省值为0.9
-   RMSprop使用的${\beta}_2$常用的缺省值为0.999
-   $\varepsilon$ 常用的缺省值为$10^{-8}$
-   这三个超参数没那么重要，一般情况下，并不需要特别调试它

## 学习率衰减

------

### 思想

加快学习算法的一个办法就是随时间慢慢减少学习率，称为学习率衰减。在学习初期，能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让步伐小一些。可以更稳定地靠近优化成本函数的最小值。

### 常用函数

首先介绍几个变量

-   **decay-rate**为衰减率，是另一个需要调整的超参数
-   **epoch-num**为代数
-   $\alpha_{0}$ 为初始学习率
-   $k$ 为一个常数

$$
\alpha= \frac{1}{1 + decayrate * \text{epoch}\text{-num}}{\alpha}_{0}
$$

$$
\alpha ={0.95}^{\text{epoch-num}} {\alpha}_{0}
$$

$$
\alpha =\frac{k}{\sqrt{\text{epoch-num}}}{\alpha}_{0}
$$

也可以尝试在训练时手动衰减 $\alpha$

## 局部最优的问题

------

在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优。梯度下降法或者某个算法可能困在一个局部最优中，而不会抵达全局最优。如下图：

![困在极差的局部最优](1f7df04b804836fbcadcd258c0b55f74.png)

事实上，对于一个神经网络，成本函数是一个具有高维度空间的函数，通常梯度为零的点并不是这个图中的局部最优点，而是鞍点。因为不太容易出现每个维度都是梯度为零的情况。在这种情况下，可以在另一个维度上梯度下降。

![鞍点](c5e480c51363d55e8d5e43df1eee679b.png)


{% endraw %}
