---
title: 训练神经网络的实用技巧
date: 2023-01-30 08:58:00 +0800
categories: [深度学习]
tags: [深度学习]

img_path: "/assets/img/posts/2023-01-30-训练神经网络的实用技巧"
math: true
---

> 这属于一个文章系列，前置知识移步：[深度学习文章分类](/categories/深度学习/)
{: .prompt-tip }

{% raw %}
## 训练，验证，测试集的比例

-----

通常会将数据划分成几部分：

-   一部分作为训练集（train set），用于训练模型
-   一部分作为简单交叉验证集，有时也称之为验证集（dev set），用于评估这些模型
-   最后一部分则作为测试集（test sets），主要目的是正确评估（多种）分类器的性能

**如果有10000条数据**

常见做法是70%训练集，30%测试集。

如果明确设置了验证集，也可以按照60%训练集，20%验证集和20%测试集来划分。

**如果有100万条数据**

测试集的主要目的是正确评估分类器的性能，所以，如果拥有百万数据，我们只需要1000条数据，便足以评估单个分类器，并且准确评估该分类器的性能。

假设我们有100万条数据，其中1万条作为验证集，1万条作为测试集，即：训练集占98%，验证集和测试集各占1%。

对于数据量过百万的应用，训练集可以占到99.5%，验证和测试集各占0.25%，或者验证集占0.4%，测试集占0.1%。

**补充**

-   建议确保验证集和测试集的数据来自同一分布（数据格式与质量相似）。
-   就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。

## 偏差与方差

------

![数据集拟合](05ac08b96177b5d0aaae7b7bfea64f3a.png){: .shadow }

-   对应最左边一幅图，如果给这个数据集拟合一条直线，但它并不能很好地拟合该数据，这是高偏差（**high bias**）的情况，称为“欠拟合”（**underfitting**）。特点是训练集误差与验证集误差都较大，且误差率相似。
-   对应最右边一幅图，如果拟合一个非常复杂的分类器，可能非常适用于**这个**数据集，但是也不是一种很好的拟合方式，分类器方差较高（**high variance**），数据过度拟合（**overfitting**）。特点是训练集误差较小，验证集误差较大。
-   对应中间一幅图，复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（**just right**），是介于过度拟合和欠拟合中间的一类。特点是训练集误差与验证集误差都较小，且误差率相似。

##  正则化

------

深度学习可能存在过拟合问题——高方差，解决方法有：

1.   准备更多的数据。包括新数据和经过处理的旧数据（比如翻转，裁剪，扭曲的图片）。

     ![经过处理的旧数据](L2_week1_17.png){: .shadow }

2.   正则化（Regularization）。当获取更多数据变得困难时，正则化通常有助于避免过拟合或减少你的网络误差。

3.   实时记录代价函数$J$，在适当的时候提前结束训练。

     ![在极小值处结束训练](9d0db64a9c9b050466a039c935f36f93.png){: .shadow }

### $L2$正则化

**在逻辑回归中实现$L2$正则化**

修改代价函数，公式为：

$$
J(w,b)=\frac1m\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}\left\| w \right\|_2^2
$$

其中，$w$ 是一个列向量，$$\left\| w \right\|_2^2$$ 是 $w$ 的欧几里德范数的平方（上标代表平方，下标代表它属于 $L2$ 正则化方法），等于$w^{T}w$ 。

其中，$\lambda$ 是正则化参数，通常使用验证集或交叉验证集来配置这个参数，尝试各种各样的数据，寻找最好的参数，要考虑训练集之间的权衡，把参数设置为较小值，这样可以避免过拟合，所以 $\lambda$ 是另外一个需要调整的超级参数。

>   Q：为什么只正则化参数$w$？为什么不再加上参数 $b$ 呢？
>
>   A：当然可以正则化参数$b$。因为参数$w$包含了绝大部分参数，所以只正则化参数$w$的效果就足够了，正则化参数$b$的效果不大。

**在神经网络中实现$L2$正则化**

修改代价函数，公式为：

$$
J(w,b)=\frac1m\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda }{2m}\sum_{l=1}^{L}{||W^{\left[l\right]}||}^{2}_{F}
$$

其中，$W^{[l]}$ 是第 $l$ 层的的参数，维度为 $(n^{[l]},n^{[l-1]})$ 。

$${\left\| W^{[l]} \right\|}^{2}_{F}$$ 被称作“弗罗贝尼乌斯范数”，用下标 $F$ 标注，定义为矩阵中所有元素的平方求和。

**实现梯度下降**

>   复习一下，不加入正则化之前，$d{W^{[l]}}$ 的公式为：
>
>   $$
>   d{W^{[l]}}=\frac{1}{m}\text{}d{Z^{[l]}}\cdot {A^{\left[ l-1 \right]T}}
>   $$
>


加入正则化后，修改公式为：

$$
d{W^{[l]}}=\frac{1}{m}\text{}d{Z^{[l]}}\cdot {A^{\left[ l-1 \right]T}}+\frac{\lambda }{m}{W}^{[l]}
$$

$$
{W}^{[l]}:={W}^{[l]}-\alpha \cdot d{W^{[l]}}
$$

不论$W^{[l]}$是什么，我们都试图让它变得更小，因此$L2$正则化也被称为“权重衰减”。

>   Q：为什么正则化有利于预防过拟合呢？
>
>   A：如果正则化$\lambda$设置得足够大，权重矩阵$W$会变小到接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元。换句话说，神经网络被大大简化了，小到如同一个逻辑回归单元，它会使这个网络从过度拟合的状态变为高偏差状态。再适当设置$\lambda$，使其“适度拟合”。

### 反向随机失活正则化

**思想：**

![随机失活](9fa7196adeeaf88eb386fda2e9fa9909.png){: .shadow }

是对于全部样本的每一次训练，每次随机删除一些节点（将激活值置0），保留小网络进行训练。

**实现：**

用一个三层（$l=3$）网络来举例说明。实现在某一层中实施随机失活（**dropout**）。

1.   定义向量$d$，$d^{[3]}$表示网络第三层的随机失活向量。定义一个保留值**keep-prob**，它是一个具体数字，表示保留某个隐藏单元的概率。代码为：`d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keep-prob`
2.   从第三层中获取激活值$a^{[3]}$，令$a^{[3]}$乘以$d^{[3]}$，代码为`a3 = np.multiply(a3,d3)`
3.   最后，向外扩展$a^{[3]}$，用它除以keep-prob参数，代码为`a3 /= keep-prob`

**解释：**

-   $d^{[3]}$则是一个布尔型数组，部分为 1，部分为 0，概率取决于定义的保留值keep-prob。
-   $a^{[3]}$乘以$d^{[3]}$之后，其中的部分激活值置零，或者说**失活。**
-   $a^{[3]}$的部分激活值置零，为了不影响期望值，需要等比例扩大剩余的值，所以除以keep-prob参数。
-   每层保留值keep-prob可以不同，如果某层 $W^{[l]}$ 参数集较大，为了预防矩阵的过拟合，它的keep-prob值应该相对较低。其他层的keep-prob可以接近于1。
-   除非算法过拟合，不然不要使用随机失活正则化。

>   Q：为什么随机失活会起作用呢？
>
>   A：因为单元的每个输入都可能随时被清除，所以结果不能依赖于任何一个特征，不能给任何一个输入加上太多权重。

## 归一化

------

训练神经网络，其中一个加速训练的方法就是归一化输入，它的思想是统一每个输入特征的平均值和方差。

**实现：**

用一个具有两个输入特征的例子来说明，因为两个输入特征可以画到平面图上。

![两个输入特征的分布](L2_week1_19.png){: .shadow }

一开始的分布如最左边的图，两个输入特征的均值都不为0，并且方差不统一。

第一步是零均值化，公式为：

$$
\mu = \frac{1}{m}\sum_{i =1}^{m}x^{(i)}
$$

$$
x^{(i)}:=x^{(i)}-\mu
$$

这样，每个输入特征的平均值都为0，如中间那幅图。

第二步是归一化方差，公式为：

$$
\sigma^{2}= \frac{1}{m}\sum_{i =1}^{m}{({x^{(i)})}^{2}}
$$

$$
x^{(i)}:=x^{(i)}/\sigma^{2}
$$

这样，每个输入特征的方差都为1，如最右边那幅图。

>   Q：我们为什么要归一化输入特征呢？
>
>   A：![归一化前后对比](4d0c183882a140ecd205f1618243d7f8.png){: .shadow }
>
>   如图，在归一化输入特征之前，代价函数是非常细长狭窄的，按照梯度下降是曲折困难的，如果归一化特征，代价函数更对称，不论从哪个位置开始，梯度下降法都能够更直接地找到最小值。

## 梯度消失/梯度爆炸

------

梯度消失/梯度爆炸（Vanishing / Exploding gradients），也就是训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变大变小，这加大了训练的难度。

**原因**

假设你正在训练这样一个极深的神经网络，$\hat{y}$ 正比于 $ W^{[l]}W^{[L -1]}W^{[L - 2]}\ldots W^{[3]}W^{[2]}W^{[1]}x$。

假设每个权重矩阵$$W^{[l]} = \begin{bmatrix} 1.5 & 0 \\0 & 1.5 \\ \end{bmatrix}$$，那么 $\hat{y}$ 的值也会非常大，实际上它呈指数级增长的，它增长的比率是${1.5}^{L}$，因此对于一个深度神经网络，$\hat{y}$ 的值将爆炸式增长。

相反的，如果权重是0.5，$$W^{[l]} = \begin{bmatrix} 0.5& 0 \\ 0 & 0.5 \\ \end{bmatrix}$$，激活函数的值将以指数级下降。

**解决方法**

合理初始化权重有助于训练出一个权重或梯度不会增长或消失过快的深度网络。虽然不能彻底解决问题，却很有用。

使用一个神经元来说明。$z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}$，当$n$越大，为了控制$z$，希望$w_{i}$越小。初始化权重的公式不是唯一的，比较常见的实现有：
$$
W^{[l]} = np.random.randn( \text{shape})*\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})
$$

$$
W^{[l]} = np.random.randn( \text{shape})*\text{np.}\text{sqrt}(\frac{2}{n^{[l-1]}})
$$

$$
W^{[l]} = np.random.randn( \text{shape})*\text{np.}\text{sqrt}(\frac{2}{n^{[l-1]} + n^{\left[l\right]}})
$$

其中， $n^{[l - 1]}$ 是第$l-1$层神经元数量。第一个公式更适用于**tanh**激活函数。第二个公式更适用于**Relu**激活函数。

## 梯度检验

------

**思想**

梯度检验是一个 debug 方法，可以使用这个方法来检验反向传播是否得以正确实施，具体来讲，验证导数是否正确。

**实现**

定义向量$\theta$，它是所有参数转换成的一个巨大的向量数据，具体来说是把所有参数 $W$ 和 $b$ 转换成向量之后，做连接运算得到的。

类似定义$d\theta$，它是所有参数${dW}^{[l]}$和${db}^{[l]}$转换成的新向量，维度与 $\theta$ 相同。

计算 $d\theta\left[i \right]$ 与估计值 $d\theta_{\text{approx}}\left[i \right]$，根据导数的定义，可得：

$$
d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}
$$

$$
d\theta\left[i \right]=\frac{\partial J}{\partial\theta_{i}}
$$

$d\theta\left[i \right]$ 与估计值 $d\theta_{\text{approx}}\left[i \right]$ 应该相近，如何定义两个向量是否真的接近彼此？具体来说做下列运算：


$$
difference=\frac{{||d\theta_{\text{approx}} -d\theta||}_{2}}{||d\theta_{\text{approx}}||_{2}+||d\theta||_{2}}
$$


其中，$$\| \dots  \|_2$$ 表示欧几里得范数，或者向量长度。

-   如果值为$10^{-7}$或更小，这就很好，这就意味着导数逼近很有可能是正确的，它的值非常小。
-   如果值在$10^{-5}$范围内，小心，也许这个值没问题，建议再次检查这个向量的所有项，确保没有一项误差过大，可能这里有bug。
-   如果结果是$10^{-3}$或更大，应该仔细检查所有$\theta$项，追踪一些求导计算是否正确。

**实用技巧和注意事项**

-   不要在训练中使用梯度检验，它只用于调试，因为它会使算法变慢。
-   如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出bug。
-   在实施梯度检验时，如果使用正则化，请注意不要漏掉正则项。
-   梯度检验不能与随机失活同时使用，因为其中的部分激活值会被置零。
-   在$w$和$b$接近0时，梯度下降的实施不容易发现错误，但是当$W$和$b$变大时，它会变得越来越不准确。如果随机初始化值比较小，反复训练网络之后，再重新运行梯度检验。

{% endraw %}
