---
title: 浅层神经网络的结构与运算
date: 2023-01-29 16:26:00 +0800
categories: [深度学习]

img_path: "/assets/img/posts/2023-01-29-浅层神经网络的结构与运算"
math: true
---

> 这属于一个文章系列，前置知识移步：[深度学习文章分类](/categories/深度学习/)
{: .prompt-tip }

## 神经网络结构

------

![一个简单神经的网络结构](L1_week3_3.png){: .shadow }

许多sigmoid单元堆叠起来形成一个神经网络。

1. 输入特征$x_1$、$x_2$、$x_3$，它们被竖直地堆叠起来，这叫做神经网络的**输入层**。它包含了神经网络的输入
2. 另外一层我们称之为**隐藏层**（中间四个结点）。这些中间结点的准确值我们是不知道到的
3. 最后一层只由一个结点构成，而这个只有一个结点的层被称为**输出层**，它负责产生预测值

## 正向传播

------

使用上标符号$^{[m]}$表示第$m$层网络中节点相关的数，这些节点的集合被称为第$m$层网络。

>  复习一下：上标符号$^{(i)}$表示第$i$个训练样本

在约定俗成的符号传统中，对于这个例子，只能叫做一个两层的神经网络。原因是输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。输入层称为第零层。

对于隐藏层，第一个单元或结点我们将其表示为$$a^{[1]}_{1}$$，第二个结点的值我们记为$a^{[1]}_{2}$，以此类推，所以这里的是一个四维的列向量。
$$
a^{[1]} =
	\left[
		\begin{matrix}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{matrix}
	\right]
$$

### 一个样本的单结点计算

对于$a^{[1]}_{1}$节点，把上一层（一个输入样本）的所有输出作为输入进行计算，并且有对应的参数 $w^{[1]}_1$ （一个列向量，维度为：输入数量 * 1 ）和参数 $b^{[1]}_1$ （一个数），根据之前学习的逻辑回归，有公式：
$$
z^{[1]}_{1}=w^{[1]T}_1x+b^{[1]}_1
$$

$$
a^{[1]}_{1}=\sigma (z^{[1]}_{1})
$$

### 一个样本的一层结点计算

现在把同一层的结点组合起来，有公式：
$$
\left[
    \begin{array}{}
    a^{[1]}_{1}\\
    a^{[1]}_{2}\\
    a^{[1]}_{3}\\
    a^{[1]}_{4}
    \end{array}
\right]
=
\left[
    \begin{array}{}
    \sigma(z^{[1]}_{1})\\
    \sigma(z^{[1]}_{2})\\
    \sigma(z^{[1]}_{3})\\
    \sigma(z^{[1]}_{4})
    \end{array}
\right]
$$

$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...w^{[1]T}_{1}...\\
		...w^{[1]T}_{2}...\\
		...w^{[1]T}_{3}...\\
		...w^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$

简洁一点来写：
$$
a^{[1]}=\sigma(z^{[1]})
$$

$$
z^{[1]}=W^{[1]}x+b^{[1]}
$$

- 参数 $W^{[1]}$ 是一个维度为（本层结点数 ， 输入数量）的矩阵
- $z^{[1]}$，$a^{[1]}$，参数 $b^{[1]}$ 是一个维度为（本层结点数 ，1）的列向量
- $x$ 为输入，是一个维度为（输入数量，1）的列向量

同样的，对于输出层：
$$
\hat{y}=a^{[2]}=\sigma(z^{[2]})
$$

$$
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}
$$

### 全部样本的一层结点计算

现在把所有样本组合起来，符号定义和维度为：

$$
\underbrace{X}_{(特征数量，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

$$
\underbrace{Z^{[1]}}_{(本层结点数，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

$$
\underbrace{A^{[1]}}_{(本层结点数，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		\alpha^{[1](1)} & \alpha^{[1](2)} & \cdots & \alpha^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

$$
\underbrace{B^{[1]}}_{(本层结点数，样本数量)} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		b^{[1](1)} & b^{[1](2)} & \cdots & b^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
	\right]
$$

> 事实上，对于全部样本的一次计算，各层的参数 $W$ 和参数 $b$ 是不变的，全部样本计算完才计算梯度
>
> 也就是说，$$b^{[1](1)}=b^{[1](2)}=\cdots  b^{[1](m)}$$
>
> 所以矩阵 $B^{[1]}$ 可以用列向量 $b^{[1]}$ 简化表示，python在计算时会自动使用**广播**技术填充

> 可以这么理解：对于$X,Z,A,B$这些矩阵的任意一个，矩阵内部左右移动为样本的选择，上下移动为一层结点的选择

于是有公式：
$$
A^{[1]} = \sigma(Z^{[1]})
$$

$$
Z^{[1]} = W^{[1]}X + B^{[1]}=W^{[1]}A^{[0]} + B^{[1]}
$$

$$
A^{[2]}=\sigma(Z^{[2]})
$$

$$
Z^{[2]}=W^{[2]}A^{[1]}+B^{[2]}
$$

## 激活函数

------

![四种激活函数](L1_week3_9.jpeg){: .shadow }

到目前为止，只用过**sigmoid**激活函数，但是，有时其他的激活函数效果会更好。这里介绍并比较4种

> Q : 为什么需要非线性的激活函数？换句话说是否可以去掉激活函数这一步？
>
> A : 非线性的激活函数是必要的。否则不管神经网络有多少层，都会训练出线型的函数。换句话说，我们希望我们的神经网络处理复杂的任务，这个函数不复杂不行，对吧？

### sigmoid函数

除了输出层是一个二分类问题（$\hat{y}$ 的数值介于0和1之间）基本不会用它，性能比较差。
$$
a=g(z)=\frac{1}{1 + e^{-z}}
$$

$$
\frac{d}{dz}g(z) = {\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))
$$

### tanh函数

$$
g(z)= tanh(z) = \frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}
$$

$$
\frac{d}{dz}g(z) = 1 - (tanh(z))^{2}
$$

事实上，**tanh**函数是**sigmoid**的向下平移和伸缩后的结果。**tanh**函数在所有场合都优于**sigmoid**函数，因为其输出的平均值接近于零，因此它可以更好地将数据集中到下一层。

**sigmoid**函数和**tanh**函数两者共同的缺点是，在 $z$ 特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。

### Relu函数

$$
g(x) =max( 0,z)
$$

$$
g(z)^{'}=
  \begin{cases}
  0&	\text{if z < 0}\\
  1&	\text{if z > 0}\\
undefined&	\text{if z = 0}
\end{cases}
$$

只要$z$是正值的情况下，导数恒等于1，当$z$是负值的时候，导数恒等于0。

**Relu**是最常用的激活函数，如果不确定用哪个激活函数，就使用**ReLu**。

### Leaky ReLu函数

$$
a = max( 0.01z,z)
$$

$$
g(z)^{'}=
\begin{cases}
0.01& 	\text{if z < 0}\\
1&	\text{if z > 0}\\
undefined&	\text{if z = 0}
\end{cases}
$$

常数不一定是0.01，可以为学习算法选择不同的参数。

## 神经网络计算流程

------

1. 初始化参数 $W^{[1]}, W^{[2]},b^{[1]},b^{[2]}$
2. 对所有样本，由参数 $W^{[1]}, W^{[2]},b^{[1]},b^{[2]}$ 计算预测值 $\hat{y}$
3. 计算损失 $J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = {\frac{1}{m}}\sum_{i=1}^mL(\hat{y}, y)$
4. 计算导数 $dW^{[1]}, dW^{[2]},db^{[1]},db^{[2]}$ ，并更新参数
5. 回到步骤 2 进行下一轮计算，直到模型可被接受

> Q1：为什么在初始化时要将参数设置为随机数？不能全为0吗？
>
> A1：如果全设置为 0 ，同一层的各个节点参数就是一样的，并且在之后的训练中，同一层的各个节点参数仍然保持一致，这是我们不愿意看到的情况。对于笔记开始的那个神经网络来讲，可以这么写：
> $$
> W^{[1]} = np.random.randn(2,2)\;*\;0.01\;,\;b^{[1]} = np.zeros((2,1))
> $$
>
> $$
> W^{[2]} = np.random.randn(2,2)\;*\;0.01\;,\;b^{[2]} = 0
> $$
>
> Q2：为什么要乘 0.01？
>
> A2：对于sigmoid函数和tanh函数，如果参数 $W$ 过大，激活函数的输入也非常大，从而导致梯度接近于零，换句话说就是“学得慢”。所以我们希望参数 $W$ 能接近 0

## 反向传播与导数计算

------

$$
dZ^{[2]}=A^{[2]}-Y
$$

$$
dW^{[2]}={\frac{1}{m}}dZ^{[2]}{A^{[1]}}^{T}
$$

$$
db^{[2]} = {\frac{1}{m}}\sum dZ^{[2]}
$$

对应代码：`db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)`

> 这里`np.sum`是python的numpy命令，`axis=1`表示水平相加求和，`keepdims`是防止python输出那些古怪的秩数$(n,)$，加上这个确保阵矩阵 $db^{[2]}$ 这个向量输出的维度为$(n,1)$这样标准的形式。
>
> 还有一种防止python输出奇怪的秩数，需要显式地调用`reshape`把`np.sum`输出结果写成矩阵形式。

$$
\underbrace{dZ^{[1]}}_{(n^{[1]}, m)} = \underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]}, m)}*\underbrace{g[1]^{'}(Z^{[1]})}_{(n^{[1]}, m)}
$$

> $*$ 代表进行逐个元素乘积

$$
dW^{[1]} = {\frac{1}{m}}dZ^{[1]}X^{T}
$$


$$
db^{[1]} = {\frac{1}{m}}\sum dZ^{[1]}
$$

对应代码：`db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)`

> ## 问题：
>
> 1. 我可以感性地理解，层数越多，隐藏层节点越多，能处理的问题就可以更复杂，这种神经网络的复杂性能不能被量化？（如果神经网络的复杂性可以与现实问题的复杂性建立起联系，那么可以更好地指导找到隐藏层数和节点数）
> 2. 神经网络只是“记住”它遇到的样本吗？它是否在做更智能的事？我们能从训练结果学习到什么吗？
