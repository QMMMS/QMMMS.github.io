---
title: 深度学习中的逻辑回归
date: 2023-01-28 12:45:00 +0800
categories: [深度学习]
tags: [深度学习, 逻辑回归]

# img_path: "/assets/img/posts/2023-01-28-Git与团队合作"
math: true
---

## 介绍

------

逻辑回归是一个用于二分类(**binary classification**)的算法。

> 这里有一个二分类问题的例子，假如你有一张图片作为输入，比如猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。

**符号定义 ：**

- $x$ ：表示一个 $n_x$ 维数据，为输入数据，维度为 $( n_x , 1 )$ 的列向量
- $y$ ：表示输出结果，取值为 $( 0 , 1 )$
- $\hat{y}$ ：算法预测值，有时会用 $a$ 表示
- $m$ ：表示样本数目
- $(x^{(i)},y^{(i)})$ ：表示第 $i$ 组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据
- $X=[x^{(1)},x^{(2)},\dots,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x * m$ 的矩阵中
- $Y=[y^{(1)},y^{(2)},\dots,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为 $1*m$

## 目标

------

基本想法是让 $\hat{y}=w^Tx+b$ （参数 $w$ 为列向量），但是这对于二元分类问题来讲不是一个非常好的算法，对于概率来说 $\hat{y} $  应该在0到1之间，但是 $w^Tx+b$ 可能比1要大得多，或者甚至为一个负值。因此在逻辑回归中，$\hat{y}$ 应该是等于由上面得到的线性函数式子作为自变量的sigmoid函数中，公式如下面所示，将线性函数转换为非线性函数。
$$
\sigma(z)=\frac 1{1+e^{-z}}
$$

$$
z=w^Tx+b
$$

$$
\hat{y}=\sigma(z)=\sigma(w^Tx+b)
$$

此时，参数 $w$ 和参数 $b$ 是未知的，需要找到一种方法来找到参数 $w$ 和参数 $b$

## 损失函数

------

损失函数又叫做误差函数，用来衡量预测输出值和实际值有多接近。平方差就是一个不错的损失函数，但是在逻辑回归模型中会定义另外一个损失函数：
$$
L(\hat{y},y)=-y\log(\hat{y})-(1-y)\log(1-\hat{y})
$$

## 代价函数

------

为了训练逻辑回归模型的参数 $w$ 和参数 $b$，我们需要一个代价函数，通过训练代价函数来得到参数 $w$ 和参数 $b$。损失函数只适用于单个训练样本，而代价函数是参数的总代价。
$$
J(w,b)=\frac1m\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})
$$
所以在训练逻辑回归模型时候，目标是需要找到合适的参数 $w$ 和参数 $b$ ，来让代价函数 $J(w,b)$ 的总代价降到最低。

## 梯度下降法

------

之前使用特殊的损失函数$L(\hat{y},y)$，是为了让代价函数 $J(w,b)$ 成为凸函数。整个梯度下降法的迭代过程就是不断地得到参数 $w$ 和参数 $b$ ，不断地向最小值点方向走。

> 凸函数的形状？一个碗。
>
> 梯度下降法就像在碗的边缘，找到一个最大下降的方向降到碗底。

具体对参数 $w$ 和参数 $b$ 更新的公式如下，其中 $:=$ 代表更新，$\alpha$ 代表学习率，$\partial$ 代表求偏导。
$$
w := w- \alpha \frac{\partial J(w,b)}{\partial w}
$$

$$
b := b - \alpha \frac{\partial J(w,b)}{\partial b}
$$

ok，现在的目标是计算参数 $w$ 和参数 $b$ 的偏导数。

## 具体计算

------

目标是计算参数 $w$ 和参数 $b$ 的偏导数，整理一下思路，参数 $w$ 和参数 $b$ 是这样到达损失函数的：
$$
z =w^Tx+b= w_1 x_1 + w_2 x_2 + b \Rightarrow \hat{y}=a=\sigma(z) \Rightarrow L(a,y)
$$
根据求导的链式法则，反方向一层一层求导。
$$
\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a}
$$

$$
\frac{da}{dz}=\frac{e^{-z}}{(1+e^{-z})^2}=a(1-a)
$$

$$
\frac{dL}{dz}=\frac{dL}{da}\cdot\frac{da}{dz}=a-y
$$

$$
\frac{\partial L}{\partial w}=\frac{\partial z}{\partial w}\cdot\frac{dL}{dz}=x(a-y)
$$

以上对于单个训练样本的简要求导步骤，如果考虑多个样本（代价函数），结论是：
$$
dw=\frac1m\sum_i^m x^{(i)}(a^{(i)}-y^{(i)})
$$

$$
db=\frac1m\sum_i^m(a^{(i)}-y^{(i)})
$$

## 实现过程与优化

------

整个训练过程可以概括为：对于这一轮的参数 $w$ 和参数 $b$ ，计算估计值 $a$。再根据这一轮的结果改进参数 $w$ 和参数 $b$ 。进行下一轮计算……在一轮轮计算之后，参数 $w$ 和参数 $b$ 会越来越适合，误差越来越小，一直到结果可被接受。

**进行一轮计算的伪代码：**

```
J=0, dw=np.zeros(n_x), db=0)	# 初始化
for i = 1 to m					# 对m个样本遍历
    z(i) = wx(i)+b
    a(i) = sigmoid(z(i))
    J += -y(i)log(a(i))-(1-y(i))log(1-a(i))
    dz(i) = a(i)-y(i)
    dw += x(i)dz(i)
    db += dz(i);
J /= m, dw /= m, db /= m
w=w-alpha*dw					#更新
b=b-alpha*db
```

> 参数 $w$ 和数据 $x$ 维度都为 $( n_x , 1 )$，要计算 $w^Tx$ ，或者说向量点乘，`numpy`中的`dot`函数可以快速做到这一点，使用举例：
>
> ```python
> import numpy as np
> a = np.array([1,2,3,4])
> b = np.array([1,2,3,4])
> c = np.dot(a,b)
> print(c)# c = 30
> ```
>
> `np.zeros(n_x)`生成一个维度为 $( n_x , 1 )$的全为0的向量

**优化：使用矩阵**
$$
Z=[z^{(1)}z^{(2)}\cdots z^{(m)}]=w^TX+[bb\cdots b]
$$
对应代码：`Z = np.dot(w.T,X) + b`

> - `dot`函数对于矩阵来说计算矩阵相乘
> - `w.T`将向量 $w$ 转置成行向量
> - `b`是一个数，在python中会自动转化为 $1 * m$ 的行向量，称为**广播**

$$
A=\sigma(Z)
$$

$$
dZ=A-Y
$$

$$
dw=\frac1m X dZ^T
$$

$$
db=\frac1m \sum dZ
$$

对应代码：`db = np.sum(dZ) / m`

> `sum`函数对一个向量求和

> ## 问题：
>
> 1. $z=w^Tx+b$ ， $L(\hat{y},y)$ ，$\sigma(z)$ 是不可替代的函数吗？
> 2. 向量和矩阵（例如$Z=w^TX+b$ 和 $dw=\frac1m X dZ^T$）可以优化计算时间，有没有更深层的含义或者合理性?
> 3. 参数 $w$ 和参数 $b$ 有没有现实的可解释的意义？
