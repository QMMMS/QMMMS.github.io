---
title: 训练 MSVQA 与分析
date: 2024-02-19 8:21:00 +0800

img_path: "/assets/img/posts/2024-02-19-训练MSVQA"
categories: [深度学习]
tags: [视频问答,实验]
---

我们模型的代码仓库：[Gitee](https://gitee.com/horizon-mind/msvqa)，首先需要获取项目源代码并且放到深度学习服务器上。

## 准备数据集

- [**MSVD-QA**](https://mega.nz/#!QmxFwBTK!Cs7cByu_Qo42XJOsv0DjiEDMiEm8m69h60caDYnT_PQ)下载网址（需要上网下载）
- 视频文件下载[网站](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/)，拉到底选择[YouTubeClips.tar](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/YouTubeClips.tar)
- [youtube_mapping.txt](https://mega.nz/#!QrowUADZ!oFfW_M5wAFsfuFDEJAIa2BeFVHYO0vxit3CMkHFOSfw) 下载网址（需要上网下载）

步骤：

- 第一个链接会下载下来一个压缩包，解压，里面有一个`video`文件夹。
- 把第二个链接下载的所有视频放到`video`文件夹下。
- `youtube_mapping.txt`放在MSVD-QA文件夹下。
- 将整个MSVD-QA文件夹上传到服务器代码文件夹下的`data`文件夹下。

MSVD-QA文件夹结构：

```
.
│  readme.txt
│  test_qa.json
│  train_qa.json
│  val_qa.json
│  youtube_mapping.txt
│ 
└─video
        -4wsuPCjDBc_5_15.avi
        -7KMZQEsJW4_205_208.avi
        -8y1Q0rA3n8_108_115.avi
        -8y1Q0rA3n8_95_102.avi
```

## 预处理

### 处理视觉特征

首先从[pretrained model](https://drive.google.com/drive/folders/1zvl89AgFAApbH0At-gMuZSeQB_LpNP-M)下载 resnext-101-kinetics.pth，这是resnext的权重文件，之后就不用预训练了。将`.pth`文件放入`data/`文件夹中。

```bash
python preprocess/preprocess_videos.py --annotation_file data/msvd-qa/train_qa.json --model resnet101

python preprocess/preprocess_videos.py --annotation_file data/msvd-qa/train_qa.json --model resnext101

python preprocess/preprocess_videos.py --annotation_file data/msvd-qa/val_qa.json --model resnet101

python preprocess/preprocess_videos.py --annotation_file data/msvd-qa/val_qa.json --model resnext101

python preprocess/preprocess_videos.py --annotation_file data/msvd-qa/test_qa.json --model resnet101

python preprocess/preprocess_videos.py --annotation_file data/msvd-qa/test_qa.json --model resnext101
```

### 处理问题

下载[glove pretrained 300d word vectors](http://nlp.stanford.edu/data/glove.840B.300d.zip) 到`data/glove/` 然后处理为 pickle 文件:

```bash
python txt2pickle.py
```

然后处理问题：

```bash
python preprocess/preprocess_questions.py --dataset msvd-qa --glove_pt data/glove/glove.840.300d.pkl --mode train

python preprocess/preprocess_questions.py --dataset msvd-qa --mode val

python preprocess/preprocess_questions.py --dataset msvd-qa --mode test
```

> --output_dir /root/autodl-fs

做完这一步之后，`data`文件夹下应该如下：

```
/msvqa/data# ls
glove  msvd-qa  resnext-101-kinetics.pth
```

```
msvqa/data/msvd-qa# ls
frames                      msvd-qa_val_questions.pt  train_qa.json  video
msvd-qa_train_questions.pt  msvd-qa_vocab.json        val_qa.json    youtube_mapping.txt
```

## 训练模型

在`options`文件夹中修改训练参数，然后：

```bash
python train.py --msvd_data_path ./data/msvd_qa_data
```

## 实验结果

所有的实验我们使用一块 RTX 3090 完成。

`tf-logs`文件夹存放了TensorBoard可视化结果，可以通过如下命令看到实验结果：

```bash
tensorboard --logdir tf-logs
```

> 一个小教程：[TensorBoard简单使用](https://qmmms.github.io/posts/TensorBoard%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/)

对应的参数可以查看仓库[附件](https://gitee.com/horizon-mind/msvqa/releases/tag/v0.1)存放实验结果，只保留了自动记录的控制台消息`console.log`。与`tf-logs`文件夹中的结果对应，包含：

- standard2
- standard3
- standard4

**第一次实验（standard1）**一共训练了 3866 个批次，batch_size=8，耗时 1小时24分钟。第一次实验是试验性的，并没有使用自编码器来优化编码器部分，更接近参考模型 DualVGR。第一次实验并没有跑到收敛，最后的 avg_loss=5.342，avg_acc=0.1913。

![](standard1.png)

**第二次实验（standard2）**一共训练了 3万 个批次，batch_size=10，耗时 20小时26分钟。其中，首先跑了300批次的自编码器来优化编码器部分，最后损失收敛到1000左右。（注意自编码器图上没有求平均，batch_size=10，总损失为 1万）

![](sd2ae.png)

然后是平均准确率和平均损失，一共训练了 3万 个批次，batch_size=10，耗时 20小时26分钟。虽然在训练集上准确率仍然有上升的趋势，但是从验证集上看，准确率已经趋于稳定因此没有进一步的训练。

![](sd2acc.png)

![](sd2v.png)

有几个点可以分析：

- 相比于 DualVGR，在去掉自编码器以外，仅仅是将 ResNet 与 ResNext 加入训练的区别。然而使用了更多参数之后，训练结果没有变好。
- 在训练集的准确度和验证集的准确度上出现了很大的差别，意味着模型过拟合了。
- 注意到在训练集上的准确率和损失，在一个 epoch 结束后会发生一次跳跃式的改变，尚没有搞清楚背后的原因是什么。
- 在加入更多参数之后，显存容量变得捉襟见肘，训练的时间也大大增长了。注意到一个 epoch 大约为 3000 batch，因此只训练了 10 个 epoch 就需要 20 小时，而原来的 DualVGR 训练 10 个 epoch  只需要 14 分钟。

**第三次实验（standard3）**是从第二次实验的 1.5万 批次开始继续实验，一直训练到 6万 批次。由于是继续实验，没有使用自编码器来优化编码器部分。与第二次实验不同的是，第二次实验学习率为 1e-5，第三次实验学习率为 1e-4

![](sd3acc.png)

![](sd3v.png)

同样的，虽然在训练集上准确率仍然有上升的趋势，但是从验证集上看，准确率已经趋于稳定，因此没有进一步的训练。

由于第三次实验是从第二次实验继续实验得来的，我们将两次实验放在一起看：

![](sd23acc.png)

此外，小组对使用自编码器来优化编码器部分还进行了一些实验，结果如下：

| id | image_height | image_width | batch_size | num_frames_per_clip | lr       | layer2 | layer3   | layer4   | inner_dim | z_size=768 | loss                        | val_loss    |
| ------------ | ------------ | ----------- | ---------- | ------------------- | -------- | ------ | -------- | -------- | --------- | ---------- | --------------------------- | ----------- |
| 1 | 100          | 100         | 8          | 16                  | 0.00001  | z-512  | 512-256  | 256-512  | 512       | 768        | min（8302）\|\|avg（10000） | min（1199） |
| 2 | 128          | 128         | 8          | 16                  | 0.00001  | z-512  | 512-1024 | 1024-512 | 512       | 768        | 6578\|\|10000               | 1227        |
| 3 | 100          | 100         | 8          | 16                  | 0.000025 | z-512  | 512-1024 | 1024-512 | 512       | 768        | 6077\|\|10000               | 1161        |
| 4 | 100          | 100         | 8          | 16                  | 0.00001  | z-256  | 256-512  | 521-256  | 512       | 768        | 7424\|\|10000               | 1086        |

![](msid1.png)

![](msid2.png)

![](msid3.png)

![](msid4.png)

## 代码分析

我们完整的模型图如下：

![](msvqa.png)

### 代码组织

- `data`文件夹存放数据集
- `data_laoder`文件夹存放数据加载器和数据集类
- `img`文件夹存放参考图片
- `model`文件夹存放模型代码，是工作的核心
- `options`文件夹存放复杂处理命令行参数的类
- `preprocess`文件夹存放预处理视频和问题文本的类
- `results`文件夹存放实验结果
- `tf-logs`文件夹存放TensorBoard可视化结果
- `util`文件夹存放代码中用到的小工具
- `train.py`训练模型

### 一些细节

预处理视频特征主要是将视频切片，转换为 tensor。将一个视频进行均匀地切分，每个视频表示为 $c$ 个连续的片段，每个片段包含了 $f$ 帧的图像，图图像的像素大小固定为$h×w$，因为是彩色视频所以通道数 channel = 3。

如果切对应ResNet101的视频，结果是torch.Size([c, f, channel, w, h])。如果切对应ResNext101的视频，结果是torch.Size([c, channel, f, w, h])

预处理问题主要将自然语言问题使用Glove编码成为tensor。

模型有两个优化方式：第一个是传统的视频问答计算损失和优化模型；第二个是通过自编码器重构视频来优化编码器Encoder，提升编码效果。

其余可以参考：

- [DualVGR](https://qmmms.github.io/posts/DualVGR-A-Dual-Visual-Graph-Reasoning-Unit/)
- [基于知识蒸馏的视频问答模型](https://qmmms.github.io/posts/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9A%84%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%94%E6%A8%A1%E5%9E%8B/)
- [训练 DualVGR](https://qmmms.github.io/posts/%E8%AE%AD%E7%BB%83DualVGR/)

### 总结&未来展望

此项目为**2023年省级大学生创新创业训练计划**，基于深度学习的视频问答题目的模型。

这个模型本质是DualVGR一个小小的改进，具体可以看上面的参考链接，几个模型的架构都是比较相似的。不如说当我再次回顾原论文时发现，我们做的这么一些小小的改进，对比原论文的工作来说，好比只是在一栋大楼上加了一个小小的装饰。

我们预定的工作计划是在原来的模型上加上自编码器与对抗生成网络。思路受[对抗的不完全多视图聚类(AIMC)方法](https://qmmms.github.io/posts/AIMC/)启发，AIMC的模型图如下：

![](liucheng.png)

在代码上，自编码器与对抗生成网络架构的一个很好的参考是 [VIGAN](https://qmmms.github.io/posts/VIGAN/)，模型如下：

![](all.png)

项目完成时，完成了对整个项目的重构（DualVGR）、自编码器部分代码编写、以及初步的实验。由于效果一般，并且预估对抗生成网络的加入也不会对模型效果起到显著作用，对抗生成网络的计划被无限搁置了。如果要为这个不算成功的项目找一些原因，应该是下面这些：

- 人员专业能力不强。对论文的理论部分，特别是一些早期深度学习论文涉及大量数学知识，对于这些部分的理解不到位或者搞不懂，一知半解。
- 人员动手能力不强。深度学习的成果建立于大量的实验之上，这种动手能力既需要坚实的代码功底，也需要对深度学习框架的熟悉。这种动手能力需要在一个又一个实战项目中不断磨练。
- 优先级。作为本科生而言，最重要的且花费时间最多的是自己的绩点，做大创以及科研，在大二时期只能排在第二位。在很多时候，我们什么都要，但是我们没有什么都要的能力，就不得不面临“鱼与熊掌”的问题。
- 交流。在做大创时主要的外援有两个，一个是学长，一个是导师，然而在最后总结的时候来看，我们的交流太少了。导师和学长都有自己的工作，很难会关注大创，需要我们自己主动去联系他们做交流。而我作为负责人不是很外向，更喜欢自己去探索，也许失去了一些纠正方向的机会。
- 前期快速验证计划的缺失。为了验证一个思路是否可行，可以快速实现一个小模型来验证。不过也正是由于之前的原因，导致无法快速实现验证用的小模型。在确认思路走不通的时候，已经浪费了太多时间。

此项目是[我们小组](https://gitee.com/horizon-mind)用于学习、练手与实验的第一个项目。作为一个大创项目，以及用于练手的项目，很难能做出SOTA的结果。在深度学习发展如此迅速的今天，在这个大创项目周期还没有结束的时候(2024.02.18)，这个模型已经远远落后于时代了。

但，当我们把眼光放得小一点，只把它当做一个大创项目来说，它的工作量已经足够了，也让我们学习到了深度学习的前沿知识，领略到了深度学习的魅力，为我们后续的工作与学习奠定了基础。
